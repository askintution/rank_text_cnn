{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Input, Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.layers.merge import Concatenate, Dot\n",
    "from keras import regularizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ques_max_len = 20\n",
    "ques_vocab_size = 1000\n",
    "ans_max_len = 40\n",
    "ans_vocab_size = 1000\n",
    "embedding = np.random.rand(1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare layers for Question\n",
    "input_q = Input(shape=(ques_max_len,))\n",
    "# print input_q.name\n",
    "# we will load embedding values from corpus here.\n",
    "embed_q = Embedding(input_dim=ques_vocab_size, output_dim=50, input_length=ques_max_len,\n",
    "                    weights=[embedding], trainable=False)(input_q)\n",
    "# Padding means, if input size is 32x32, output will also be 32x32, i.e, the dimensions\n",
    "# will not reduce\n",
    "conv_q = Conv1D(filters=100, kernel_size=5, strides=1, padding='same',\n",
    "                activation='relu', kernel_regularizer=regularizers.l2(1e-5))(embed_q)\n",
    "# also referenced as x(q) in paper\n",
    "pool_q = GlobalMaxPooling1D()(conv_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare layers for Answer\n",
    "input_a = Input(shape=(ans_max_len,))\n",
    "# print input_a.name\n",
    "embed_a = Embedding(input_dim=ans_vocab_size, output_dim=50, input_length=ans_max_len,\n",
    "                    weights=[embedding], trainable=False)(input_a)\n",
    "conv_a = Conv1D(filters=100, kernel_size=5, strides=1, padding='same',\n",
    "                activation='relu', kernel_regularizer=regularizers.l2(1e-5))(embed_a)\n",
    "pool_a = GlobalMaxPooling1D()(conv_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# M or the similarity layer here\n",
    "# Paper: x_d_dash = M.x_d\n",
    "x_a = Dense(100, use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(pool_a)\n",
    "sim = Dot(axes=-1)([pool_q, x_a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine Question, sim and Answer pooled outputs.\n",
    "join_layer = keras.layers.concatenate([pool_q, sim, pool_a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using relu here too? Not mentioned in the paper.\n",
    "hidden_layer = Dense(201, kernel_regularizer=regularizers.l2(1e-4))(join_layer)\n",
    "hidden_layer = Dropout(0.5)(hidden_layer)\n",
    "\n",
    "# Final Softmax Layer, add regularizer here too?\n",
    "softmax = Dense(1, activation='softmax')(hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_19 (InputLayer)            (None, 40)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_18 (InputLayer)            (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)         (None, 40, 50)        50000       input_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)         (None, 20, 50)        50000       input_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)               (None, 40, 100)       25100       embedding_17[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)               (None, 20, 100)       25100       embedding_16[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (GlobalM (None, 100)           0           conv1d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (GlobalM (None, 100)           0           conv1d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 100)           10000       global_max_pooling1d_16[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dot_6 (Dot)                      (None, 1)             0           global_max_pooling1d_15[0][0]    \n",
      "                                                                   dense_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 201)           0           global_max_pooling1d_15[0][0]    \n",
      "                                                                   dot_6[0][0]                      \n",
      "                                                                   global_max_pooling1d_16[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 201)           40602       concatenate_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 201)           0           dense_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 1)             202         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 201,004\n",
      "Trainable params: 101,004\n",
      "Non-trainable params: 100,000\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input_q, input_a], outputs=softmax)\n",
    "print model.summary()\n",
    "\n",
    "model.compile(optimizer='Adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# model.fit([data_a, data_b], labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
